{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2ueYfnZ9867"
   },
   "source": [
    "# Investigating RNNs and RL using the N-back cognitive task\n",
    "\n",
    "**NMA 2023 Group 1 Project**\n",
    "\n",
    "__Content creators:__ Aland Astudillo, Campbell Border, Disheng, Julia Yin, Koffivi\n",
    "\n",
    "__Pod TA:__ Suryanarayanan Nagar Anthel Venkatesh\n",
    "\n",
    "__Project Mentor:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Objective\n",
    "\n",
    "- \n",
    "\n",
    "- \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwxGrBIy-jz4"
   },
   "source": [
    "# Project Design\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb2cp2_wCSCI"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6eFAloFCZog",
    "outputId": "d2f4791b-85e9-4176-8a30-c205f952f451"
   },
   "outputs": [],
   "source": [
    "# # @title Install dependencies\n",
    "# %pip install jedi --quiet\n",
    "# %pip install --upgrade pip setuptools wheel --quiet\n",
    "# %pip install numpy==1.23.3 --quiet --ignore-installed\n",
    "# %pip install gymnasium --quiet\n",
    "# %pip install torch --quiet\n",
    "# %pip install matplotlib --quiet\n",
    "# %pip uninstall seaborn -y --quiet\n",
    "# %pip install seaborn --quiet\n",
    "# #!pip install trfl --quiet\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pEza9W8KDK1y"
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "\n",
    "#import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "#import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import math\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "Zb0JSXLiECA6"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "from IPython.display import clear_output, display, HTML\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Background\n",
    "\n",
    "## Replace with our own literature review\n",
    "\n",
    "- Cognitive scientists use standard lab tests to tap into specific processes in the brain and behavior. Some examples of those tests are Stroop, N-back, Digit Span, TMT (Trail making tests), and WCST (Wisconsin Card Sorting Tests).\n",
    "\n",
    "- Despite an extensive body of research that explains human performance using descriptive what-models, we still need a more sophisticated approach to gain a better understanding of the underlying processes (i.e., a how-model).\n",
    "\n",
    "- Interestingly, many of such tests can be thought of as a continuous stream of stimuli and corresponding actions, that is in consonant with the RL formulation. In fact, RL itself is in part motivated by how the brain enables goal-directed behaviors using reward systems, making it a good choice to explain human performance.\n",
    "\n",
    "- One behavioral test example would be the N-back task.\n",
    "\n",
    "  - In the N-back, participants view a sequence of stimuli, one by one, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedback is given at both timestep and trajectory levels.\n",
    "\n",
    "  - The agent is rewarded when its response matches the stimulus that was shown N steps back in the episode. A simpler version of the N-back uses two-choice action schema, that is match vs non-match. Once the present stimulus matches the one presented N step back, then the agent is expected to respond to it as being a `match`.\n",
    "\n",
    "\n",
    "- Given a trained RL agent, we then find correlates of its fitted parameters with the brain mechanisms. The most straightforward composition could be the correlation of model parameters with the brain activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "- Human Connectome Project Working Memory (HCP WM) task ([NMA-CN HCP notebooks](https://github.com/NeuromatchAcademy/course-content/tree/master/projects/fMRI))\n",
    "\n",
    "1200 subjects, each subject experience 8 blocks of 2-back and 8 blocks of 0-back.\n",
    "\n",
    "## N-back Tasks\n",
    "\n",
    "In the N-back task, participants view a sequence of stimuli, one per time, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedbacks are given at both timestep and trajectory levels.\n",
    "\n",
    "In a typical neuro setup, both accuracy and response time are measured, but here, for the sake of brevity, we focus only on accuracy of responses.\n",
    "\n",
    "- 2 back working memory task:\n",
    "\n",
    "The second condition is a 2-Back condition. During such a block, the subject is presented with a sequence of 10 images and must respond if each image is identical to the one 2 positions earlier or not (figure, right). At the beginning of the block there is a cue screen informing the subject that the upcoming stimuli are part of the 2-Back protocol. The timing of the cue screen, the presentation of the 10 stimulus images and of the response interval are identical to that of the 0-Back condition.\n",
    "\n",
    "- 0-back control memory task:\n",
    "\n",
    "The first is a match-to-sample condition (termed in the following text as 0-Back) during which a cue “Target” image is presented at the beginning of a block and which the subject has been instructed to memorize. Then a sequence of 10 images is presented. \n",
    "\n",
    "\n",
    "Any dataset that used cognitive tests would work.\n",
    "Question: limit to behavioral data vs fMRI?\n",
    "Question: Which stimuli and actions to use?\n",
    "classic tests can be modeled using 1) bounded symbolic stimuli/actions (e.g., A, B, C), but more sophisticated one would require texts or images (e.g., face vs neutral images in social stroop dataset)\n",
    "The HCP dataset from NMA-CN contains behavioral and imaging data for 7 cognitive tests including various versions of N-back.\n",
    "\n",
    "\n",
    "Details of the tMEG Working Memory task\n",
    "\n",
    "Working memory is assessed using an N-back task in which participants are asked to monitor\n",
    "sequentially presented pictures. Participants are presented with blocks of trials that consisted of\n",
    "pictures of tools or faces. Within each run, the 2 different stimulus types are presented in\n",
    "separate blocks. Also, within each run, ½ of the blocks use a 2-back working memory task and\n",
    "½ use a 0-back working memory task (as a working memory comparison). Participants are\n",
    "instructed to press a button for every picture. If the currently presented picture matches the\n",
    "cued picture (0-Back) or the same picture that was presented two pictures before (2-Back),\n",
    "subjects press one button with their right index finger. For non-matching pictures, participants\n",
    "press a second button with their right middle finger. Two runs are performed, 16 blocks each,\n",
    "with a bright fixation \"rest\" on dark background for 15 seconds between blocks. \n",
    "\n",
    "- Special modelling of images (we are probably not going to model this)\n",
    "\n",
    "There are 2 different categories of images used in this experiment: images of faces and tools.\n",
    "Each block contains images from a single category. Some of the images in the non-matched\n",
    "trials have been characterized as “Lure”. These images have been selected so that they have\n",
    "common features with the target image, but are still different. These trials as flagged as “Lure”.\n",
    "I\n",
    "\n",
    "\n",
    "https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf Page 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddUxN-0M-842"
   },
   "source": [
    "---\n",
    "## Implementation scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "The following cell implments N-back envinronment, that we later use to train a RL agent on human data. It is capable of performing two kinds of simulation:\n",
    "- rewards the agent once the action was correct (i.e., a normative model of the environment).\n",
    "- receives human data (or mock data if you prefer), and returns what participants performed as the observation. This is more useful for preference-based RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0-62uNYF-_9j"
   },
   "outputs": [],
   "source": [
    "# @title Define environment\n",
    "# N-back environment\n",
    "oh1=F.one_hot(torch.arange(0,10),num_classes=10)\n",
    "oh1=np.concatenate([np.array(oh1),np.ones([1,10])],axis=0)\n",
    "class NBack(gym.Env):\n",
    "\n",
    "    # Examples\n",
    "    # N = 2\n",
    "    # step_count =        [ 0  1  2  3  4  5  6 ]\n",
    "    # sequence =          [ a  b  c  d  a  d  a ] (except these are usually digits between 0-9)\n",
    "    # correct actions =   [ ~  ~  0  0  0  1  1 ]\n",
    "\n",
    "    # actions =           [ ~  ~   1  0  0  1  0 ]\n",
    "    # reward_class =      [ ~  ~  FP TN TN TP FN]\n",
    "    # reward =            [ ~  ~  -1  0  0  1 -1]\n",
    "\n",
    "  # Rewards input is structured as (TP, TN, FP, FN) (positive being matches)\n",
    "  def __init__(self, N=2, num_trials=25, num_targets=None, rewards=(1, 1, -1, -1), obs_length=1, seed=2023):\n",
    "\n",
    "    self.N = N\n",
    "    self.num_trials = num_trials\n",
    "    self.episode_length = num_trials + self.N\n",
    "    self.num_targets = num_targets\n",
    "    self.rewards = rewards\n",
    "    self.obs_length = obs_length\n",
    "    super().reset(seed=seed)\n",
    "\n",
    "    # Check that parameters are legal\n",
    "    assert(len(rewards) == 4)\n",
    "    assert(num_targets is None or num_targets <= num_trials)\n",
    "\n",
    "    # Define rewards, observation space and action space \n",
    "    self.reward_range = (min(rewards), max(rewards))  # Range of rewards based on inputs\n",
    "    self.observation_space = spaces.Discrete(10)      # Single variable with 10 possibilities if using digits or 26 if using letters\n",
    "    self.action_space = spaces.Discrete(2)            # 0 (No match) or 1 (Match)\n",
    "\n",
    "  def reset(self, seed=None):\n",
    "\n",
    "    # Seed RNG\n",
    "    super().reset(seed=seed)\n",
    "\n",
    "    # Generate sequence and correct actions\n",
    "    self._generate_sequence()\n",
    "    self._get_correct_actions()\n",
    "\n",
    "    # Observation is first character\n",
    "    self.step_count = 0\n",
    "    observation = self._get_observation()\n",
    "    \n",
    "\n",
    "    return observation, self.step_count, None, False\n",
    "\n",
    "  def step(self, action):\n",
    "\n",
    "    # Calculate label\n",
    "    label=self._get_correct_actions()\n",
    "\n",
    "    # Return next character or None\n",
    "    self.step_count += 1\n",
    "    if self.step_count < self.num_trials+2*self.N-self.obs_length:\n",
    "      return self._get_observation(), self.step_count, label, False\n",
    "    else:\n",
    "      return self._get_observation(), self.step_count, label, True\n",
    "\n",
    "  def _generate_sequence(self):\n",
    "\n",
    "    # Generate sequence of length self.episode_length (with correct number of targets)\n",
    "    while True:\n",
    "      self.sequence = np.random.randint(0, 9, size=(self.episode_length))\n",
    "      if not self.num_targets or sum(self._get_correct_actions()) == self.num_targets:\n",
    "        break\n",
    "  def _get_observation(self):\n",
    "    self.padded_sequence=np.concatenate([10*np.ones(self.N),self.sequence])\n",
    "    self.observation=self.padded_sequence[self.step_count:self.step_count+self.obs_length]\n",
    "    # self.observation1h=np.zeros(10*self.obs_length)\n",
    "    # for iob,ob in enumerate(self.observation):\n",
    "    #     self.observation1h[10*iob:10*(iob+1)]=np.array(oh1[int(ob),:])\n",
    "    self.observation1h=np.array([(oh1[int(ob),:]) for ob in self.observation])\n",
    "\n",
    "    return self.observation1h\n",
    "\n",
    "  def _get_correct_actions(self):\n",
    "    self.correct_actions= np.concatenate([np.zeros(int(2*self.N)),np.array([int(self.sequence[i] == self.sequence[i + self.N]) for i in range(self.num_trials)])])\n",
    "    self.correct_actions1hot=np.zeros([self.num_trials+2*self.N,2])\n",
    "    for i,act in enumerate(self.correct_actions):\n",
    "        if act==1:\n",
    "            self.correct_actions1hot[i,:]=[1,0]\n",
    "        else:\n",
    "            self.correct_actions1hot[i,:]=[0,1]\n",
    "\n",
    "\n",
    "    return self.correct_actions1hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define a decentralised supervised learning MLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class SupMLP(nn.Module):\n",
    "    def __init__(self, num_obs, num_actions,RNN_hidden_sizes=None, RNN_layers=None ,lin_hidden_sizes=None, actv=\"ReLU()\"):\n",
    "        super().__init__()\n",
    "\n",
    "        if lin_hidden_sizes is None:\n",
    "            lin_hidden_sizes = []\n",
    "        if RNN_hidden_sizes is None:\n",
    "            RNN_hidden_sizes = 0\n",
    "        self.RNN_layers=RNN_layers\n",
    "        self.RNN_hidden_sizes=RNN_hidden_sizes\n",
    "        self.num_obs = num_obs\n",
    "        self.num_actions = num_actions\n",
    "        self.lin_hidden_sizes = lin_hidden_sizes\n",
    "\n",
    "        # Create net\n",
    "        prev_size = 10*self.num_obs # Initialize the temporary input feature to each layer\n",
    "        self.RNNlayer = nn.RNN(input_size=prev_size,hidden_size=RNN_hidden_sizes,num_layers=RNN_layers)\n",
    "        prev_size=RNN_hidden_sizes\n",
    "        self.linlayer = nn.Sequential()\n",
    "\n",
    "\n",
    "        for i in range(len(lin_hidden_sizes)): # Loop over layers and create each one\n",
    "\n",
    "            # Add linear layer\n",
    "            current_size = lin_hidden_sizes[i] # Assign the current layer hidden unit from list\n",
    "            layer = nn.RNN(prev_size, current_size)\n",
    "            prev_size = current_size # Assign next layer input using current layer output\n",
    "            self.linlayer.add_module('RNN_%d'%i, layer) # Append layer to the model\n",
    "\n",
    "            # Add activation function\n",
    "            actv_layer = eval('nn.%s'%actv) # Assign activation function (eval allows us to instantiate object from string)\n",
    "            self.linlayer.add_module('Activation_%d'%i, actv_layer) # Append activation to the model with a name\n",
    "\n",
    "        out_layer = nn.Linear(prev_size, self.num_actions) # Create final layer\n",
    "        self.linlayer.add_module('Output_Linear', out_layer) # Append the final layer\n",
    "        sm=nn.Softmax()\n",
    "        self.linlayer.add_module('softmax',sm)\n",
    "    def forward(self,x,state):\n",
    "        x,state=self.RNNlayer(x,state)\n",
    "        x=self.linlayer(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class SUPagent():\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        self.num_obs = env.obs_length\n",
    "        self.num_actions = 2\n",
    "\n",
    "        self.supnw=SupMLP(self.num_obs, self.num_actions,RNN_hidden_sizes=32,RNN_layers=1,lin_hidden_sizes=[128])\n",
    "        self.lr = 0.08\n",
    "        self.momentum = 0.8\n",
    "\n",
    "        self.steps_done = 0\n",
    "        self.optimizer=torch.optim.SGD(self.supnw.parameters(),lr=self.lr,momentum=self.momentum)\n",
    "        self.loss_fn=torch.nn.CrossEntropyLoss()\n",
    "    def choose_action(self,seq):\n",
    "        return self.supnw(torch.tensor(seq)).max(1)[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def getepisodes(env,num_episodes):\n",
    "\n",
    "    Len_padded=env.num_trials+2*env.N\n",
    "    episodes_X=torch.zeros(num_episodes,Len_padded,10*env.obs_length)\n",
    "    episodes_Y=torch.zeros(num_episodes,Len_padded,2)\n",
    "    for n in range(num_episodes):\n",
    "        obs, _, _, done = env.reset()\n",
    "        episodes_Y[n,:,:]=torch.tensor(env.correct_actions1hot)\n",
    "        episodes_X[n,env.step_count,0:10*env.obs_length]=torch.tensor(obs)\n",
    "        while not done:\n",
    "            obs, _, _, done = env.step(0)\n",
    "            if obs is not None:\n",
    "                episodes_X[n,env.step_count,0:10*env.obs_length]=torch.tensor(obs)\n",
    "    return episodes_X,episodes_Y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def shuffle_and_split_data(X, y):\n",
    "  \"\"\"\n",
    "  Helper function to shuffle and split incoming data\n",
    "\n",
    "  Args:\n",
    "    X: torch.tensor\n",
    "      Input data\n",
    "    y: torch.tensor\n",
    "      Corresponding target variables\n",
    "    seed: int\n",
    "      Set seed for reproducibility\n",
    "\n",
    "  Returns:\n",
    "    X_test: torch.tensor\n",
    "      Test data [20% of X]\n",
    "    y_test: torch.tensor\n",
    "      Labels corresponding to above mentioned test data\n",
    "    X_train: torch.tensor\n",
    "      Train data [80% of X]\n",
    "    y_train: torch.tensor\n",
    "      Labels corresponding to above mentioned train data\n",
    "  \"\"\"\n",
    "  # Number of samples\n",
    "  N = X.shape[0]\n",
    "  # Shuffle data\n",
    "  shuffled_indices = torch.randperm(N)   # Get indices to shuffle data, could use torch.randperm\n",
    "  X = X[shuffled_indices]\n",
    "  y = y[shuffled_indices]\n",
    "\n",
    "  # Split data into train/test\n",
    "  test_size = int(0.2 * N)    # Assign test datset size using 20% of samples\n",
    "  X_test = X[:test_size]\n",
    "  y_test = y[:test_size]\n",
    "  X_train = X[test_size:]\n",
    "  y_train = y[test_size:]\n",
    "  # y_train_total=torch.numel(y_train)/2\n",
    "  # y_train_match=torch.sum(y_train,dim=0)\n",
    "  # print(y_train_match,y_train_total,y_train_match/y_train_total)\n",
    "  return X_test, y_test, X_train, y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def train_test_classification(net, criterion, optimizer, train_loader,\n",
    "                              test_loader, num_epochs=1, verbose=True,\n",
    "                              training_plot=False, device=device):\n",
    "  \"\"\"\n",
    "  Accumulate training loss/Evaluate performance\n",
    "\n",
    "  Args:\n",
    "    net: instance of Net class\n",
    "      Describes the model with ReLU activation, batch size 128\n",
    "    criterion: torch.nn type\n",
    "      Criterion combines LogSoftmax and NLLLoss in one single class.\n",
    "    optimizer: torch.optim type\n",
    "      Implements Adam algorithm.\n",
    "    train_loader: torch.utils.data type\n",
    "      Combines the train dataset and sampler, and provides an iterable over the given dataset.\n",
    "    test_loader: torch.utils.data type\n",
    "      Combines the test dataset and sampler, and provides an iterable over the given dataset.\n",
    "    num_epochs: int\n",
    "      Number of epochs [default: 1]\n",
    "    verbose: boolean\n",
    "      If True, print statistics\n",
    "    training_plot=False\n",
    "      If True, display training plot\n",
    "    device: string\n",
    "      CUDA/GPU if available, CPU otherwise\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "\n",
    "  def test(data_loader):\n",
    "    \"\"\"\n",
    "    Function to gauge network performance\n",
    "\n",
    "    Args:\n",
    "      data_loader: torch.utils.data type\n",
    "      Combines the test dataset and sampler, and provides an iterable over the given dataset.\n",
    "\n",
    "    Returns:\n",
    "      acc: float\n",
    "        Performance of the network\n",
    "      total: int\n",
    "        Number of datapoints in the dataloader\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    error=0\n",
    "    once=1\n",
    "    for data in data_loader:\n",
    "      inputs, labels = data\n",
    "      inputs = inputs.to(device).float()\n",
    "      labels = labels.to(device).float()\n",
    "\n",
    "      for ep in range(inputs.size(dim=0)):\n",
    "          ep_input=inputs[ep,:,:]\n",
    "          ep_label=labels[ep,:,:]\n",
    "      # forward + backward + optimize\n",
    "          ep_output = net(ep_input)\n",
    "\n",
    "          outputs_bl=np.zeros_like(ep_output.cpu().detach().numpy())\n",
    "          for num,output in enumerate(ep_output):\n",
    "              if output[0]>=output[1]:\n",
    "                  outputs_bl[num,:]=[1,0]\n",
    "              else:\n",
    "                  outputs_bl[num,:]=[0,1]\n",
    "          outputs_bl_tensor=torch.from_numpy(outputs_bl).to(device).float()\n",
    "          correct += (outputs_bl_tensor[:,0] == ep_label[:,0]).sum().item()\n",
    "      # while once==1:\n",
    "      #       print(outputs,outputs_bl)\n",
    "      #       once+=1\n",
    "          total += ep_label.size(0)\n",
    "          print(ep_label.size(1))\n",
    "\n",
    "      outputs_bl=torch.tensor(outputs_bl).to(device).float()\n",
    "      # error+=outputs_bl[:,0] -ep_label[:,0]\n",
    "    acc=1\n",
    "    acc = 100 * correct / total\n",
    "\n",
    "    # print(error)\n",
    "\n",
    "    return total, acc\n",
    "\n",
    "\n",
    "\n",
    "  net.train()\n",
    "  training_losses = []\n",
    "  state=torch.zeros([net.RNNlayer,net.RNN_hidden_sizes])\n",
    "  for epoch in tqdm(range(num_epochs)):  # Loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "      # Get the inputs; data is a list of [inputs, labels]\n",
    "      inputs, labels = data\n",
    "      inputs = inputs.to(device).float()\n",
    "      labels = labels.to(device).float()\n",
    "\n",
    "      # Zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "      for ep in range(inputs.size(dim=0)):\n",
    "          ep_input=inputs[ep,:,:]\n",
    "          ep_label=labels[ep,:,:]\n",
    "          ep_output=torch.zeros_like(ep_label)\n",
    "          i=0\n",
    "          for inpt,lbl in ep_input,ep_label:\n",
    "      # forward + backward + optimize\n",
    "            outpt,state = net(inpt,state)\n",
    "            ep_output[i,:]=outpt\n",
    "            i+=1\n",
    "      # if epoch==0:\n",
    "      #     print(inputs,labels,outputs)\n",
    "      #     print(type(outputs))\n",
    "          loss = criterion(ep_output, ep_label)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "      # Print statistics\n",
    "          if verbose:\n",
    "              training_losses += [loss.item()]\n",
    "        # print(training_losses)\n",
    "\n",
    "  net.eval()\n",
    "\n",
    "\n",
    "\n",
    "  train_total, train_acc = test(train_loader)\n",
    "  test_total, test_acc = test(test_loader)\n",
    "\n",
    "  if verbose:\n",
    "    print(f\"Accuracy on the {train_total} training samples: {train_acc:0.2f}\")\n",
    "    print(f\"Accuracy on the {test_total} testing samples: {test_acc:0.2f}\")\n",
    "\n",
    "  if training_plot:\n",
    "    plt.plot(training_losses)\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Training loss')\n",
    "    plt.show()\n",
    "\n",
    "  return train_acc, test_acc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 7 2 8 6 0]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "<class 'numpy.ndarray'>\n",
      "0 [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "1 [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] <class 'numpy.ndarray'>\n",
      "2 [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] <class 'numpy.ndarray'>\n",
      "3 [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]] <class 'numpy.ndarray'>\n",
      "4 [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] <class 'numpy.ndarray'>\n",
      "5 [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]] <class 'numpy.ndarray'>\n",
      "6 [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]] <class 'numpy.ndarray'>\n",
      "7 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] <class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Test environment\n",
    "env = NBack(N=2, num_trials=4, obs_length=1)\n",
    "obs, _, _, done = env.reset()\n",
    "print(env.sequence)\n",
    "print(env.correct_actions1hot)\n",
    "print(type(env.correct_actions))\n",
    "print(env.step_count, obs)\n",
    "while not done:\n",
    "  obs, _, _, done = env.step(0)\n",
    "\n",
    "  print(env.step_count, obs,type(obs))\n",
    "for ob in obs:\n",
    "    print(type(ob))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 1., 0.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.]]]) tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         ...,\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         ...,\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         ...,\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         ...,\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         ...,\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         ...,\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]]]) torch.Size([100, 8, 10]) torch.Size([100, 8, 2])\n",
      "1 tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 1., 0.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]) tensor([[[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         ...,\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         ...,\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         ...,\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         ...,\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         ...,\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         ...,\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]]) torch.Size([100, 8, 10]) torch.Size([100, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "X,Y=getepisodes(env,num_episodes=1000)\n",
    "X_test, y_test, X_train, y_train=shuffle_and_split_data(X,Y)\n",
    "# print(X_test.size(),y_test)\n",
    "batch_size = 100\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size,\n",
    "                         shuffle=False, num_workers=0,\n",
    "                         worker_init_fn=seed_worker)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, drop_last=True,\n",
    "                          shuffle=True, num_workers=0,\n",
    "                          worker_init_fn=seed_worker)\n",
    "for i, data in enumerate(test_loader):\n",
    "    x,y=data\n",
    "    print(i,x,y,np.shape(x),np.shape(y))\n",
    "# net = SUPagent(env).supnw.to(device)\n",
    "# for data in train_loader:\n",
    "#       inputs, labels = data\n",
    "#       print(inputs,labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SupMLP(\n",
      "  (RNNlayer): RNN(10, 32)\n",
      "  (linlayer): Sequential(\n",
      "    (RNN_0): RNN(32, 128)\n",
      "    (Activation_0): ReLU()\n",
      "    (Output_Linear): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (softmax): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zeros(): argument 'size' (position 1) must be tuple of ints, but found element of type RNN at pos 0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(net\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5e-4\u001B[39m)\n\u001B[0;32m      5\u001B[0m num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[1;32m----> 7\u001B[0m _, _ \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_classification\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mtraining_plot\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[18], line 87\u001B[0m, in \u001B[0;36mtrain_test_classification\u001B[1;34m(net, criterion, optimizer, train_loader, test_loader, num_epochs, verbose, training_plot, device)\u001B[0m\n\u001B[0;32m     85\u001B[0m net\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     86\u001B[0m training_losses \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 87\u001B[0m state\u001B[38;5;241m=\u001B[39m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mRNNlayer\u001B[49m\u001B[43m,\u001B[49m\u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mRNN_hidden_sizes\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(num_epochs)):  \u001B[38;5;66;03m# Loop over the dataset multiple times\u001B[39;00m\n\u001B[0;32m     89\u001B[0m   running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n",
      "\u001B[1;31mTypeError\u001B[0m: zeros(): argument 'size' (position 1) must be tuple of ints, but found element of type RNN at pos 0"
     ]
    }
   ],
   "source": [
    "net = SUPagent(env).supnw.to(device)\n",
    "print(net)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=5e-4)\n",
    "num_epochs = 10\n",
    "\n",
    "_, _ = train_test_classification(net, criterion, optimizer, train_loader,\n",
    "                                 test_loader, num_epochs=num_epochs,\n",
    "                                 training_plot=True, device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
